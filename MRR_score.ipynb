{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-13T19:27:04.367061Z",
     "iopub.status.busy": "2025-01-13T19:27:04.366713Z",
     "iopub.status.idle": "2025-01-13T19:27:19.130163Z",
     "shell.execute_reply": "2025-01-13T19:27:19.129471Z",
     "shell.execute_reply.started": "2025-01-13T19:27:04.367027Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel,AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util,SentenceTransformer, SentenceTransformerTrainer, losses, SentenceTransformerTrainingArguments\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bước 1: Đọc dataset NLI và QA sau đó tách content ra làm file Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-13T19:27:24.363902Z",
     "iopub.status.busy": "2025-01-13T19:27:24.363601Z",
     "iopub.status.idle": "2025-01-13T19:27:24.369217Z",
     "shell.execute_reply": "2025-01-13T19:27:24.368372Z",
     "shell.execute_reply.started": "2025-01-13T19:27:24.363878Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(data):\n",
    "    \"\"\"\n",
    "    Xử lý chuỗi để:\n",
    "    - Giữ lại phần text trong [**text**] hoặc [text].\n",
    "    - Bỏ phần (https://...).\n",
    "    \n",
    "    Args:\n",
    "        data (str): Chuỗi cần xử lý.\n",
    "    \n",
    "    Returns:\n",
    "        str: Chuỗi đã được làm sạch.\n",
    "    \"\"\"\n",
    "    # Bước 1: Lấy phần text trong [**text**] hoặc [text]\n",
    "    data = re.sub(r'\\[\\*\\*(.*?)\\*\\*\\]|\\[(.*?)\\]', lambda m: m.group(1) or m.group(2), data)\n",
    "    \n",
    "    # Bước 2: Loại bỏ phần (https://...)\n",
    "    data = re.sub(r'\\(https?://[^\\)]+\\)', '', data)\n",
    "    \n",
    "    # Loại bỏ các khoảng trắng thừa (nếu có)\n",
    "    return data.strip()\n",
    "\n",
    "# splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "# df_NLI = pd.read_parquet(\"hf://datasets/BookingCare/ViHealth-NLI/\" + splits[\"train\"])\n",
    "# splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet', 'val': 'data/val-00000-of-00001.parquet'}\n",
    "# df_QA = pd.read_parquet(\"hf://datasets/BookingCare/ViHealthQA/\" + splits[\"train\"])\n",
    "# df_QA_test = pd.read_parquet(\"hf://datasets/BookingCare/ViHealthQA/\" + splits[\"test\"])\n",
    "# df_QA_val = pd.read_parquet(\"hf://datasets/BookingCare/ViHealthQA/\" + splits[\"val\"])\n",
    "\n",
    "# # Áp dụng hàm xử lý cho cột `context`\n",
    "# df_NLI[\"sentence2\"] = df_NLI[\"sentence2\"].apply(clean_text)\n",
    "# df_QA[\"document\"] = df_QA[\"document\"].apply(clean_text)\n",
    "# df_QA_test[\"document\"] = df_QA_test[\"document\"].apply(clean_text)\n",
    "# df_QA_val[\"document\"] = df_QA_val[\"document\"].apply(clean_text)\n",
    "\n",
    "# # 1. Lấy dữ liệu từ cột document và sentence2\n",
    "# documents = df_QA['document']\n",
    "# documents1 = df_QA_test['document']\n",
    "# documents2 = df_QA_val['document']\n",
    "# sentences = df_NLI['sentence2']\n",
    "\n",
    "# # 2. Nối dữ liệu hai cột lại với nhau\n",
    "# # Ghép cả hai cột thành một DataFrame mới\n",
    "# corpus = pd.DataFrame({\n",
    "#     'context': pd.concat([documents,documents1,documents2, sentences]).reset_index(drop=True)\n",
    "# })\n",
    "\n",
    "# # # 3. Lưu vào file CSV hoặc TXT\n",
    "# corpus.to_csv('/kaggle/working/corpus.csv', index=False)  # Lưu thành file CSV\n",
    "# # df_QA.to_csv('/kaggle/working/train.csv', index=False)\n",
    "# # df_QA_test.to_csv('/kaggle/working/test.csv', index=False)\n",
    "# # df_QA_val.to_csv('/kaggle/working/val.csv', index=False)\n",
    "\n",
    "# # print(\"Dữ liệu đã được lưu vào file corpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T19:27:29.518087Z",
     "iopub.status.busy": "2025-01-13T19:27:29.515769Z",
     "iopub.status.idle": "2025-01-13T19:27:44.728419Z",
     "shell.execute_reply": "2025-01-13T19:27:44.727395Z",
     "shell.execute_reply.started": "2025-01-13T19:27:29.518015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Corpus = pd.read_csv(\"/kaggle/input/healthcare-chatbot/corpus_healthcare.csv\")\n",
    "train = pd.read_csv(\"/kaggle/input/healthcare-chatbot/train_healthcare.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/healthcare-chatbot/test_healthcare.csv\")\n",
    "val = pd.read_csv(\"/kaggle/input/healthcare-chatbot/val_healthcare.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***corpus dài nhất có: 4437 token***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T19:27:51.250108Z",
     "iopub.status.busy": "2025-01-13T19:27:51.249826Z",
     "iopub.status.idle": "2025-01-13T19:28:05.338544Z",
     "shell.execute_reply": "2025-01-13T19:28:05.337670Z",
     "shell.execute_reply.started": "2025-01-13T19:27:51.250088Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-6b0eabe49f61>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  doc_embeddings = torch.load(\"/kaggle/input/halong_embedding/other/default/1/doc_embeddings_hl_finetune.pt\")\n"
     ]
    }
   ],
   "source": [
    "bi_encoder = SentenceTransformer(\"/kaggle/input/halong_embedding_4jan/other/default/1/checkpoint-3500\").cuda()\n",
    "\n",
    "answers = list(Corpus[\"context\"])\n",
    "\n",
    "# Chỉ giữ các phần tử là chuỗi\n",
    "answers = [str(answer) for answer in Corpus[\"context\"] if isinstance(answer, (str, int, float))]\n",
    "\n",
    "question ='''Vì sao phải gây mê khi chụp MRI cho trẻ?'''\n",
    "\n",
    "# query_embedding = bi_encoder.encode(question, convert_to_tensor=True)\n",
    "# doc_embeddings = bi_encoder.encode(answers, convert_to_tensor=True)\n",
    "\n",
    "doc_embeddings = torch.load(\"/kaggle/input/halong_embedding/other/default/1/doc_embeddings_hl_finetune.pt\")\n",
    "\n",
    "# torch.save(doc_embeddings, '/kaggle/working/doc_embeddings_hl_ft.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bước 3: Cros-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T19:28:16.744660Z",
     "iopub.status.busy": "2025-01-13T19:28:16.744361Z",
     "iopub.status.idle": "2025-01-13T19:28:34.584790Z",
     "shell.execute_reply": "2025-01-13T19:28:34.584018Z",
     "shell.execute_reply.started": "2025-01-13T19:28:16.744639Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Kiểm tra nếu GPU có sẵn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Bước 1: Tải mô hình Bi-Encoder\n",
    "en = \"/kaggle/input/bge_finetune_6file/transformers/default/1/bge_reranker_finetune_24Dec_latest5\"  # \"/kaggle/input/viranker/other/26_dec/1\"\n",
    "\n",
    "cross_encoder = AutoModelForSequenceClassification.from_pretrained(en)\n",
    "cross_encoder = cross_encoder.to(device)   # Chuyển Cross-Encoder sang GPU\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TÍNH MRR BIENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T15:52:26.648191Z",
     "iopub.status.busy": "2025-01-13T15:52:26.647918Z",
     "iopub.status.idle": "2025-01-13T15:52:26.661204Z",
     "shell.execute_reply": "2025-01-13T15:52:26.660392Z",
     "shell.execute_reply.started": "2025-01-13T15:52:26.648163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\n",
    "def calculate_mrr(relevant_indices):\n",
    "    \"\"\"\n",
    "    Tính Mean Reciprocal Rank (MRR) cho bi-encoder.\n",
    "    \n",
    "    Parameters:\n",
    "    - relevant_indices: Danh sách chỉ mục các câu trả lời đúng (list of int)\n",
    "    \n",
    "    Returns:\n",
    "    - mrr: Giá trị Mean Reciprocal Rank (float)\n",
    "    \"\"\"\n",
    "    # Encode queries và corpus\n",
    "    query_embeddings = torch.load(\"/kaggle/input/query_embedding_hl/other/default/2/query_embeddings_hl_ft.pt\")[:5000]\n",
    "    corpus_embeddings = torch.load(\"/kaggle/input/halong_embedding/other/default/1/doc_embeddings_hl_finetune.pt\")\n",
    "    \n",
    "    # Chuyển embeddings từ GPU sang CPU nếu cần\n",
    "    query_embeddings = query_embeddings.cpu().numpy() if query_embeddings.is_cuda else query_embeddings.numpy()\n",
    "    corpus_embeddings = corpus_embeddings.cpu().numpy() if corpus_embeddings.is_cuda else corpus_embeddings.numpy()\n",
    "    \n",
    "    # Tính similarity scores giữa query và corpus\n",
    "    scores = cosine_similarity(query_embeddings, corpus_embeddings)\n",
    "    \n",
    "    # Tính Reciprocal Rank cho từng query\n",
    "    reciprocal_ranks = []\n",
    "    for i, relevant_idx in enumerate(relevant_indices):\n",
    "        query_scores = scores[i]\n",
    "        ranked_indices = np.argsort(query_scores)[::-1]\n",
    "        rank_position = np.where(ranked_indices == relevant_idx)[0][0] + 1  # Vị trí bắt đầu từ 1\n",
    "        reciprocal_ranks.append(1 / rank_position)\n",
    "    \n",
    "    # Tính Mean Reciprocal Rank\n",
    "    mrr = np.mean(reciprocal_ranks)\n",
    "    return mrr\n",
    "\n",
    "# Ví dụ sử dụng\n",
    "relevant_indices = list(range(0, 112371))\n",
    "mrr = calculate_mrr(relevant_indices[:5000])\n",
    "print(f\"MRR: {mrr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TÍNH MRR CROSSENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T19:28:39.867061Z",
     "iopub.status.busy": "2025-01-13T19:28:39.866746Z",
     "iopub.status.idle": "2025-01-13T19:39:06.546905Z",
     "shell.execute_reply": "2025-01-13T19:39:06.546169Z",
     "shell.execute_reply.started": "2025-01-13T19:28:39.867040Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import util\n",
    "\n",
    "# Giả sử bạn đã có các biến sau:\n",
    "# - train: chứa dữ liệu với các cột \"query\" và \"document\".\n",
    "# - bi_encoder: mô hình Bi-Encoder đã được huấn luyện.\n",
    "# - cross_encoder: mô hình Cross-Encoder đã được huấn luyện.\n",
    "# - doc_embeddings: embedding của toàn bộ tài liệu (answers).\n",
    "# - answers: danh sách tài liệu (corpus).\n",
    "\n",
    "# Số mẫu train để tính MRR\n",
    "n_samples = 1000\n",
    "\n",
    "def calculate_mrr(train, bi_encoder, cross_encoder, doc_embeddings, answers, n_samples):\n",
    "    mrr = 0.0\n",
    "    top_k_bi = 10\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Duyệt qua từng mẫu trong tập train\n",
    "    for idx in range(n_samples):\n",
    "        question = train[\"query\"][idx]\n",
    "        correct_answer = train[\"document\"][idx]\n",
    "\n",
    "        # 1. Bi-Encoder: Lấy top-k tài liệu\n",
    "        query_embedding = bi_encoder.encode(question, convert_to_tensor=True)\n",
    "        bi_scores = util.cos_sim(query_embedding, doc_embeddings)[0].tolist()\n",
    "        top_indices_bi = sorted(range(len(bi_scores)), key=lambda i: bi_scores[i], reverse=True)[:top_k_bi]\n",
    "        top_docs_bi = [(answers[i], bi_scores[i]) for i in top_indices_bi]\n",
    "\n",
    "        # 2. Cross-Encoder: Tái xếp hạng top-k\n",
    "        cross_doc = [[question, doc] for doc, _ in top_docs_bi]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(cross_doc, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "            outputs = cross_encoder(**inputs, return_dict=True)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.sigmoid(logits).squeeze()\n",
    "\n",
    "        # Gộp kết quả với điểm Cross-Encoder\n",
    "        top_results = [\n",
    "            (doc[0], doc[1], probabilities[i].item())\n",
    "            for i, doc in enumerate(top_docs_bi)\n",
    "        ]\n",
    "        \n",
    "        # Sắp xếp lại theo điểm Cross-Encoder\n",
    "        top_results = sorted(top_results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        # 3. Tìm vị trí của tài liệu đúng\n",
    "        rank = 0\n",
    "        for i, (doc, _, _) in enumerate(top_results):\n",
    "            if doc == correct_answer:\n",
    "                rank = i + 1\n",
    "                break\n",
    "\n",
    "        # 4. Tính Reciprocal Rank\n",
    "        if rank > 0:\n",
    "            mrr += 1.0 / rank\n",
    "        else:\n",
    "            mrr += 1.0/10.0\n",
    "\n",
    "    # 5. Tính Mean Reciprocal Rank (MRR)\n",
    "    mrr /= n_samples\n",
    "    return mrr\n",
    "\n",
    "# Gọi hàm tính MRR\n",
    "mrr_score = calculate_mrr(train, bi_encoder, cross_encoder, doc_embeddings, answers, n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T19:39:57.078146Z",
     "iopub.status.busy": "2025-01-13T19:39:57.077816Z",
     "iopub.status.idle": "2025-01-13T19:39:57.083089Z",
     "shell.execute_reply": "2025-01-13T19:39:57.082177Z",
     "shell.execute_reply.started": "2025-01-13T19:39:57.078109Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f\"MRR for the first {n_samples} samples: {mrr_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6420184,
     "sourceId": 10365783,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 208347,
     "modelInstanceId": 186229,
     "sourceId": 218375,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 208389,
     "modelInstanceId": 186274,
     "sourceId": 218425,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 215967,
     "modelInstanceId": 194051,
     "sourceId": 227609,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 215801,
     "modelInstanceId": 193885,
     "sourceId": 227611,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 216736,
     "modelInstanceId": 194835,
     "sourceId": 228485,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 216949,
     "modelInstanceId": 195053,
     "sourceId": 228738,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 217165,
     "modelInstanceId": 195272,
     "sourceId": 228994,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
