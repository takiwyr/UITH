{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-13T19:27:04.367061Z",
     "iopub.status.busy": "2025-01-13T19:27:04.366713Z",
     "iopub.status.idle": "2025-01-13T19:27:19.130163Z",
     "shell.execute_reply": "2025-01-13T19:27:19.129471Z",
     "shell.execute_reply.started": "2025-01-13T19:27:04.367027Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel,AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util,SentenceTransformer, SentenceTransformerTrainer, losses, SentenceTransformerTrainingArguments\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bước 1: Đọc dataset NLI và QA sau đó tách content ra làm file Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-13T19:27:24.363902Z",
     "iopub.status.busy": "2025-01-13T19:27:24.363601Z",
     "iopub.status.idle": "2025-01-13T19:27:24.369217Z",
     "shell.execute_reply": "2025-01-13T19:27:24.368372Z",
     "shell.execute_reply.started": "2025-01-13T19:27:24.363878Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(data):\n",
    "    \"\"\"\n",
    "    Xử lý chuỗi để:\n",
    "    - Giữ lại phần text trong [**text**] hoặc [text].\n",
    "    - Bỏ phần (https://...).\n",
    "    \n",
    "    Args:\n",
    "        data (str): Chuỗi cần xử lý.\n",
    "    \n",
    "    Returns:\n",
    "        str: Chuỗi đã được làm sạch.\n",
    "    \"\"\"\n",
    "    # Bước 1: Lấy phần text trong [**text**] hoặc [text]\n",
    "    data = re.sub(r'\\[\\*\\*(.*?)\\*\\*\\]|\\[(.*?)\\]', lambda m: m.group(1) or m.group(2), data)\n",
    "    \n",
    "    # Bước 2: Loại bỏ phần (https://...)\n",
    "    data = re.sub(r'\\(https?://[^\\)]+\\)', '', data)\n",
    "    \n",
    "    # Loại bỏ các khoảng trắng thừa (nếu có)\n",
    "    return data.strip()\n",
    "\n",
    "# splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "# df_NLI = pd.read_parquet(\"hf://datasets/BookingCare/ViHealth-NLI/\" + splits[\"train\"])\n",
    "# splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet', 'val': 'data/val-00000-of-00001.parquet'}\n",
    "# df_QA = pd.read_parquet(\"hf://datasets/BookingCare/ViHealthQA/\" + splits[\"train\"])\n",
    "# df_QA_test = pd.read_parquet(\"hf://datasets/BookingCare/ViHealthQA/\" + splits[\"test\"])\n",
    "# df_QA_val = pd.read_parquet(\"hf://datasets/BookingCare/ViHealthQA/\" + splits[\"val\"])\n",
    "\n",
    "# # Áp dụng hàm xử lý cho cột `context`\n",
    "# df_NLI[\"sentence2\"] = df_NLI[\"sentence2\"].apply(clean_text)\n",
    "# df_QA[\"document\"] = df_QA[\"document\"].apply(clean_text)\n",
    "# df_QA_test[\"document\"] = df_QA_test[\"document\"].apply(clean_text)\n",
    "# df_QA_val[\"document\"] = df_QA_val[\"document\"].apply(clean_text)\n",
    "\n",
    "# # 1. Lấy dữ liệu từ cột document và sentence2\n",
    "# documents = df_QA['document']\n",
    "# documents1 = df_QA_test['document']\n",
    "# documents2 = df_QA_val['document']\n",
    "# sentences = df_NLI['sentence2']\n",
    "\n",
    "# # 2. Nối dữ liệu hai cột lại với nhau\n",
    "# # Ghép cả hai cột thành một DataFrame mới\n",
    "# corpus = pd.DataFrame({\n",
    "#     'context': pd.concat([documents,documents1,documents2, sentences]).reset_index(drop=True)\n",
    "# })\n",
    "\n",
    "# # # 3. Lưu vào file CSV hoặc TXT\n",
    "# corpus.to_csv('/kaggle/working/corpus.csv', index=False)  # Lưu thành file CSV\n",
    "# # df_QA.to_csv('/kaggle/working/train.csv', index=False)\n",
    "# # df_QA_test.to_csv('/kaggle/working/test.csv', index=False)\n",
    "# # df_QA_val.to_csv('/kaggle/working/val.csv', index=False)\n",
    "\n",
    "# # print(\"Dữ liệu đã được lưu vào file corpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T19:27:29.518087Z",
     "iopub.status.busy": "2025-01-13T19:27:29.515769Z",
     "iopub.status.idle": "2025-01-13T19:27:44.728419Z",
     "shell.execute_reply": "2025-01-13T19:27:44.727395Z",
     "shell.execute_reply.started": "2025-01-13T19:27:29.518015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Corpus = pd.read_csv(\"/kaggle/input/healthcare-chatbot/corpus_healthcare.csv\")\n",
    "train = pd.read_csv(\"/kaggle/input/healthcare-chatbot/train_healthcare.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/healthcare-chatbot/test_healthcare.csv\")\n",
    "val = pd.read_csv(\"/kaggle/input/healthcare-chatbot/val_healthcare.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***corpus dài nhất có: 4437 token***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T15:51:47.430954Z",
     "iopub.status.busy": "2025-01-13T15:51:47.430632Z",
     "iopub.status.idle": "2025-01-13T15:51:51.338878Z",
     "shell.execute_reply": "2025-01-13T15:51:51.338094Z",
     "shell.execute_reply.started": "2025-01-13T15:51:47.430919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Giả sử DataFrame df đã có cột 'context'\n",
    "# Đếm số từ trong mỗi dòng bằng cách tách chuỗi\n",
    "Corpus[\"num_words\"] = Corpus[\"context\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Thống kê\n",
    "num_greater_512 = Corpus[Corpus[\"num_words\"] > 1024].shape[0]\n",
    "num_less_equal_512 = Corpus[Corpus[\"num_words\"] <= 1024].shape[0]\n",
    "\n",
    "# Kết quả\n",
    "print(f\"Số dòng có số từ > 1024: {num_greater_512}\")\n",
    "print(f\"Số dòng có số từ <= 1024: {num_less_equal_512}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bước 2: Bi-encoder fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T15:51:51.340018Z",
     "iopub.status.busy": "2025-01-13T15:51:51.33971Z",
     "iopub.status.idle": "2025-01-13T15:51:51.344157Z",
     "shell.execute_reply": "2025-01-13T15:51:51.343481Z",
     "shell.execute_reply.started": "2025-01-13T15:51:51.339987Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"hiieu/halong_embedding\")\n",
    "model = model.to('cuda')\n",
    "print(model.device)\n",
    "loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "def expand_dataset(df):\n",
    "    # Loại bỏ các dòng bị thiếu\n",
    "    df = df.dropna(subset=[\"query\", \"document\"])\n",
    "    \n",
    "    # Loại bỏ các giá trị không phải chuỗi\n",
    "    df = df[df[\"query\"].apply(lambda x: isinstance(x, str))]\n",
    "    df = df[df[\"document\"].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "    expanded_data = {\n",
    "        \"anchor\": df[\"query\"].tolist(),\n",
    "        \"positive\": df[\"document\"].tolist()\n",
    "    }\n",
    "\n",
    "    # Chuyển dữ liệu mở rộng thành Dataset\n",
    "    return Dataset.from_dict(expanded_data)\n",
    "\n",
    "# train = pd.read_parquet(\"hf://datasets/hungnm/vietnamese-medical-qa/data/train-00000-of-00001.parquet\")\n",
    "train_dataset = expand_dataset(train)\n",
    "subset = train_dataset.select(range(len(train_dataset))) \n",
    "\n",
    "val_dataset = expand_dataset(val)\n",
    "eval = train_dataset.select(range(len(val_dataset))) \n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "print(\"entering training cell \")\n",
    "\n",
    "from sentence_transformers import SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "\n",
    "# define training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"/kaggle/working/bi_finetune_4Jan\", # output directory and hugging face model ID\n",
    "    num_train_epochs=1,                         # number of epochs\n",
    "    per_device_train_batch_size=8,             # train batch size\n",
    "    gradient_accumulation_steps=4,             # for a global batch size of 512\n",
    "    per_device_eval_batch_size=8,              # evaluation batch size\n",
    "    #gradient_checkpointing=True,\n",
    "    warmup_ratio=0.1,                           # warmup ratio\n",
    "    learning_rate=2e-5,                         # learning rate, 2e-5 is a good value\n",
    "    lr_scheduler_type=\"cosine\",                 # use constant learning rate scheduler\n",
    "    optim=\"adamw_torch_fused\",                  # use fused adamw optimizer\n",
    "    #tf32=True,                                  # use tf32 precision\n",
    "    bf16=True,                                  # use bf16 precision\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    "    save_strategy=\"steps\",\n",
    "    # eval_strategy=\"steps\",                      # evaluate after each epoch\n",
    "    save_steps = 100,\n",
    "    logging_steps=100,                           # log every 10 steps\n",
    "    save_total_limit=2,                         # save only the last 3 models\n",
    "    metric_for_best_model=\"eval_dim_768_cosine_ndcg@10\",  # Optimizing for the best ndcg@10 score for the 128 dimension\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args, \n",
    "    train_dataset=subset,\n",
    "    loss=loss,\n",
    "    evaluator=eval,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6420184,
     "sourceId": 10365783,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 208347,
     "modelInstanceId": 186229,
     "sourceId": 218375,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 208389,
     "modelInstanceId": 186274,
     "sourceId": 218425,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 215967,
     "modelInstanceId": 194051,
     "sourceId": 227609,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 215801,
     "modelInstanceId": 193885,
     "sourceId": 227611,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 216736,
     "modelInstanceId": 194835,
     "sourceId": 228485,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 216949,
     "modelInstanceId": 195053,
     "sourceId": 228738,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 217165,
     "modelInstanceId": 195272,
     "sourceId": 228994,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
