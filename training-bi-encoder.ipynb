{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T19:26:53.004384Z",
     "iopub.status.busy": "2025-01-13T19:26:53.004087Z",
     "iopub.status.idle": "2025-01-13T19:27:00.601986Z",
     "shell.execute_reply": "2025-01-13T19:27:00.600894Z",
     "shell.execute_reply.started": "2025-01-13T19:26:53.004353Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting dataset\n",
      "  Downloading dataset-1.6.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.7)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.4.0)\n",
      "Collecting sqlalchemy<2.0.0,>=1.3.2 (from dataset)\n",
      "  Downloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: alembic>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from dataset) (1.14.0)\n",
      "Collecting banal>=1.0.1 (from dataset)\n",
      "  Downloading banal-1.0.6-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=0.6.2->dataset) (1.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=0.6.2->dataset) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (3.1.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.19.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\n",
      "Downloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
      "Downloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: banal, sqlalchemy, dataset, sentence_transformers\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.35\n",
      "    Uninstalling SQLAlchemy-2.0.35:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.35\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed banal-1.0.6 dataset-1.6.2 sentence_transformers-3.3.1 sqlalchemy-1.4.54\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-13T19:27:04.367061Z",
     "iopub.status.busy": "2025-01-13T19:27:04.366713Z",
     "iopub.status.idle": "2025-01-13T19:27:19.130163Z",
     "shell.execute_reply": "2025-01-13T19:27:19.129471Z",
     "shell.execute_reply.started": "2025-01-13T19:27:04.367027Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel,AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util,SentenceTransformer, SentenceTransformerTrainer, losses, SentenceTransformerTrainingArguments\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T19:27:21.581855Z",
     "iopub.status.busy": "2025-01-13T19:27:21.581262Z",
     "iopub.status.idle": "2025-01-13T19:27:21.769820Z",
     "shell.execute_reply": "2025-01-13T19:27:21.769059Z",
     "shell.execute_reply.started": "2025-01-13T19:27:21.581826Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bước 1: Đọc dataset NLI và QA sau đó tách content ra làm file Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-13T19:27:24.363902Z",
     "iopub.status.busy": "2025-01-13T19:27:24.363601Z",
     "iopub.status.idle": "2025-01-13T19:27:24.369217Z",
     "shell.execute_reply": "2025-01-13T19:27:24.368372Z",
     "shell.execute_reply.started": "2025-01-13T19:27:24.363878Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(data):\n",
    "    \"\"\"\n",
    "    Xử lý chuỗi để:\n",
    "    - Giữ lại phần text trong [**text**] hoặc [text].\n",
    "    - Bỏ phần (https://...).\n",
    "    \n",
    "    Args:\n",
    "        data (str): Chuỗi cần xử lý.\n",
    "    \n",
    "    Returns:\n",
    "        str: Chuỗi đã được làm sạch.\n",
    "    \"\"\"\n",
    "    # Bước 1: Lấy phần text trong [**text**] hoặc [text]\n",
    "    data = re.sub(r'\\[\\*\\*(.*?)\\*\\*\\]|\\[(.*?)\\]', lambda m: m.group(1) or m.group(2), data)\n",
    "    \n",
    "    # Bước 2: Loại bỏ phần (https://...)\n",
    "    data = re.sub(r'\\(https?://[^\\)]+\\)', '', data)\n",
    "    \n",
    "    # Loại bỏ các khoảng trắng thừa (nếu có)\n",
    "    return data.strip()\n",
    "\n",
    "# splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "# df_NLI = pd.read_parquet(\"hf://datasets/BookingCare/ViHealth-NLI/\" + splits[\"train\"])\n",
    "# splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet', 'val': 'data/val-00000-of-00001.parquet'}\n",
    "# df_QA = pd.read_parquet(\"hf://datasets/BookingCare/ViHealthQA/\" + splits[\"train\"])\n",
    "# df_QA_test = pd.read_parquet(\"hf://datasets/BookingCare/ViHealthQA/\" + splits[\"test\"])\n",
    "# df_QA_val = pd.read_parquet(\"hf://datasets/BookingCare/ViHealthQA/\" + splits[\"val\"])\n",
    "\n",
    "# # Áp dụng hàm xử lý cho cột `context`\n",
    "# df_NLI[\"sentence2\"] = df_NLI[\"sentence2\"].apply(clean_text)\n",
    "# df_QA[\"document\"] = df_QA[\"document\"].apply(clean_text)\n",
    "# df_QA_test[\"document\"] = df_QA_test[\"document\"].apply(clean_text)\n",
    "# df_QA_val[\"document\"] = df_QA_val[\"document\"].apply(clean_text)\n",
    "\n",
    "# # 1. Lấy dữ liệu từ cột document và sentence2\n",
    "# documents = df_QA['document']\n",
    "# documents1 = df_QA_test['document']\n",
    "# documents2 = df_QA_val['document']\n",
    "# sentences = df_NLI['sentence2']\n",
    "\n",
    "# # 2. Nối dữ liệu hai cột lại với nhau\n",
    "# # Ghép cả hai cột thành một DataFrame mới\n",
    "# corpus = pd.DataFrame({\n",
    "#     'context': pd.concat([documents,documents1,documents2, sentences]).reset_index(drop=True)\n",
    "# })\n",
    "\n",
    "# # # 3. Lưu vào file CSV hoặc TXT\n",
    "# corpus.to_csv('/kaggle/working/corpus.csv', index=False)  # Lưu thành file CSV\n",
    "# # df_QA.to_csv('/kaggle/working/train.csv', index=False)\n",
    "# # df_QA_test.to_csv('/kaggle/working/test.csv', index=False)\n",
    "# # df_QA_val.to_csv('/kaggle/working/val.csv', index=False)\n",
    "\n",
    "# # print(\"Dữ liệu đã được lưu vào file corpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T19:27:29.518087Z",
     "iopub.status.busy": "2025-01-13T19:27:29.515769Z",
     "iopub.status.idle": "2025-01-13T19:27:44.728419Z",
     "shell.execute_reply": "2025-01-13T19:27:44.727395Z",
     "shell.execute_reply.started": "2025-01-13T19:27:29.518015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Corpus = pd.read_csv(\"/kaggle/input/healthcare-chatbot/corpus_healthcare.csv\")\n",
    "train = pd.read_csv(\"/kaggle/input/healthcare-chatbot/train_healthcare.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/healthcare-chatbot/test_healthcare.csv\")\n",
    "val = pd.read_csv(\"/kaggle/input/healthcare-chatbot/val_healthcare.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***corpus dài nhất có: 4437 token***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T15:51:47.430954Z",
     "iopub.status.busy": "2025-01-13T15:51:47.430632Z",
     "iopub.status.idle": "2025-01-13T15:51:51.338878Z",
     "shell.execute_reply": "2025-01-13T15:51:51.338094Z",
     "shell.execute_reply.started": "2025-01-13T15:51:47.430919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Giả sử DataFrame df đã có cột 'context'\n",
    "# Đếm số từ trong mỗi dòng bằng cách tách chuỗi\n",
    "Corpus[\"num_words\"] = Corpus[\"context\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Thống kê\n",
    "num_greater_512 = Corpus[Corpus[\"num_words\"] > 1024].shape[0]\n",
    "num_less_equal_512 = Corpus[Corpus[\"num_words\"] <= 1024].shape[0]\n",
    "\n",
    "# Kết quả\n",
    "print(f\"Số dòng có số từ > 1024: {num_greater_512}\")\n",
    "print(f\"Số dòng có số từ <= 1024: {num_less_equal_512}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bước 2: Bi-encoder fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T15:51:51.340018Z",
     "iopub.status.busy": "2025-01-13T15:51:51.33971Z",
     "iopub.status.idle": "2025-01-13T15:51:51.344157Z",
     "shell.execute_reply": "2025-01-13T15:51:51.343481Z",
     "shell.execute_reply.started": "2025-01-13T15:51:51.339987Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"hiieu/halong_embedding\")\n",
    "model = model.to('cuda')\n",
    "print(model.device)\n",
    "loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "def expand_dataset(df):\n",
    "    # Loại bỏ các dòng bị thiếu\n",
    "    df = df.dropna(subset=[\"query\", \"document\"])\n",
    "    \n",
    "    # Loại bỏ các giá trị không phải chuỗi\n",
    "    df = df[df[\"query\"].apply(lambda x: isinstance(x, str))]\n",
    "    df = df[df[\"document\"].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "    expanded_data = {\n",
    "        \"anchor\": df[\"query\"].tolist(),\n",
    "        \"positive\": df[\"document\"].tolist()\n",
    "    }\n",
    "\n",
    "    # Chuyển dữ liệu mở rộng thành Dataset\n",
    "    return Dataset.from_dict(expanded_data)\n",
    "\n",
    "# train = pd.read_parquet(\"hf://datasets/hungnm/vietnamese-medical-qa/data/train-00000-of-00001.parquet\")\n",
    "train_dataset = expand_dataset(train)\n",
    "subset = train_dataset.select(range(len(train_dataset))) \n",
    "\n",
    "val_dataset = expand_dataset(val)\n",
    "eval = train_dataset.select(range(len(val_dataset))) \n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "print(\"entering training cell \")\n",
    "\n",
    "from sentence_transformers import SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "\n",
    "# define training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"/kaggle/working/bi_finetune_4Jan\", # output directory and hugging face model ID\n",
    "    num_train_epochs=1,                         # number of epochs\n",
    "    per_device_train_batch_size=8,             # train batch size\n",
    "    gradient_accumulation_steps=4,             # for a global batch size of 512\n",
    "    per_device_eval_batch_size=8,              # evaluation batch size\n",
    "    #gradient_checkpointing=True,\n",
    "    warmup_ratio=0.1,                           # warmup ratio\n",
    "    learning_rate=2e-5,                         # learning rate, 2e-5 is a good value\n",
    "    lr_scheduler_type=\"cosine\",                 # use constant learning rate scheduler\n",
    "    optim=\"adamw_torch_fused\",                  # use fused adamw optimizer\n",
    "    #tf32=True,                                  # use tf32 precision\n",
    "    bf16=True,                                  # use bf16 precision\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    "    save_strategy=\"steps\",\n",
    "    # eval_strategy=\"steps\",                      # evaluate after each epoch\n",
    "    save_steps = 100,\n",
    "    logging_steps=100,                           # log every 10 steps\n",
    "    save_total_limit=2,                         # save only the last 3 models\n",
    "    metric_for_best_model=\"eval_dim_768_cosine_ndcg@10\",  # Optimizing for the best ndcg@10 score for the 128 dimension\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args, \n",
    "    train_dataset=subset,\n",
    "    loss=loss,\n",
    "    evaluator=eval,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6420184,
     "sourceId": 10365783,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 208347,
     "modelInstanceId": 186229,
     "sourceId": 218375,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 208389,
     "modelInstanceId": 186274,
     "sourceId": 218425,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 215967,
     "modelInstanceId": 194051,
     "sourceId": 227609,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 215801,
     "modelInstanceId": 193885,
     "sourceId": 227611,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 216736,
     "modelInstanceId": 194835,
     "sourceId": 228485,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 216949,
     "modelInstanceId": 195053,
     "sourceId": 228738,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 217165,
     "modelInstanceId": 195272,
     "sourceId": 228994,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
