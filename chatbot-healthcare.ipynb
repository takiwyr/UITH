{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T01:54:22.028492Z",
     "iopub.status.busy": "2025-01-14T01:54:22.028096Z",
     "iopub.status.idle": "2025-01-14T01:54:29.695009Z",
     "shell.execute_reply": "2025-01-14T01:54:29.693942Z",
     "shell.execute_reply.started": "2025-01-14T01:54:22.028455Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install sentence_transformers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-14T01:54:35.778740Z",
     "iopub.status.busy": "2025-01-14T01:54:35.778367Z",
     "iopub.status.idle": "2025-01-14T01:54:50.254356Z",
     "shell.execute_reply": "2025-01-14T01:54:50.253681Z",
     "shell.execute_reply.started": "2025-01-14T01:54:35.778708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util,SentenceTransformer, SentenceTransformerTrainer, losses, SentenceTransformerTrainingArguments\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bước 1: Đọc dataset NLI và QA sau đó tách content ra làm file Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T19:12:53.507387Z",
     "iopub.status.busy": "2025-01-13T19:12:53.507051Z",
     "iopub.status.idle": "2025-01-13T19:13:09.020765Z",
     "shell.execute_reply": "2025-01-13T19:13:09.019845Z",
     "shell.execute_reply.started": "2025-01-13T19:12:53.507358Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Corpus = pd.read_csv(\"/kaggle/input/healthcare-chatbot/corpus_healthcare.csv\")\n",
    "train = pd.read_csv(\"/kaggle/input/healthcare-chatbot/train_healthcare.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/healthcare-chatbot/test_healthcare.csv\")\n",
    "val = pd.read_csv(\"/kaggle/input/healthcare-chatbot/val_healthcare.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***corpus dài nhất có: 4437 token***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BI-ENCODER FINETUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T01:54:57.775679Z",
     "iopub.status.busy": "2025-01-14T01:54:57.775277Z",
     "iopub.status.idle": "2025-01-14T01:54:57.779559Z",
     "shell.execute_reply": "2025-01-14T01:54:57.778496Z",
     "shell.execute_reply.started": "2025-01-14T01:54:57.775648Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import pandas as pd\n",
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T01:55:00.294734Z",
     "iopub.status.busy": "2025-01-14T01:55:00.294373Z",
     "iopub.status.idle": "2025-01-14T01:55:16.739939Z",
     "shell.execute_reply": "2025-01-14T01:55:16.739002Z",
     "shell.execute_reply.started": "2025-01-14T01:55:00.294706Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Kiểm tra xem GPU có sẵn không\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Tải mô hình và chỉ định thiết bị\n",
    "model_path = \"/kaggle/input/halong_embedding_4jan/other/default/1/checkpoint-3500\"\n",
    "model = SentenceTransformer(model_path, device=device)\n",
    "\n",
    "# Kiểm tra xem model đã được tải trên GPU hay không\n",
    "print(f\"Model device: {model.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T01:55:20.175483Z",
     "iopub.status.busy": "2025-01-14T01:55:20.175159Z",
     "iopub.status.idle": "2025-01-14T01:55:21.913748Z",
     "shell.execute_reply": "2025-01-14T01:55:21.912998Z",
     "shell.execute_reply.started": "2025-01-14T01:55:20.175458Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "import torch\n",
    "\n",
    "# Kiểm tra xem GPU có sẵn không\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Tải mô hình và chỉ định thiết bị\n",
    "model_name = \"/kaggle/input/bge_finetune_6file/transformers/default/1/bge_reranker_finetune_24Dec_latest5\"\n",
    "cross_encoder = CrossEncoder(model_name, num_labels=1, max_length=1024, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T01:55:27.738702Z",
     "iopub.status.busy": "2025-01-14T01:55:27.738322Z",
     "iopub.status.idle": "2025-01-14T01:55:27.743231Z",
     "shell.execute_reply": "2025-01-14T01:55:27.742281Z",
     "shell.execute_reply.started": "2025-01-14T01:55:27.738676Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T01:55:29.188878Z",
     "iopub.status.busy": "2025-01-14T01:55:29.188505Z",
     "iopub.status.idle": "2025-01-14T01:55:29.197524Z",
     "shell.execute_reply": "2025-01-14T01:55:29.196166Z",
     "shell.execute_reply.started": "2025-01-14T01:55:29.188836Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaForSequenceClassification(\n",
      "  (roberta): XLMRobertaModel(\n",
      "    (embeddings): XLMRobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(8194, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): XLMRobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x XLMRobertaLayer(\n",
      "          (attention): XLMRobertaAttention(\n",
      "            (self): XLMRobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): XLMRobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): XLMRobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): XLMRobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): XLMRobertaClassificationHead(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_cross = cross_encoder.model\n",
    "\n",
    "# Print the model architecture\n",
    "print(model_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T01:55:32.858220Z",
     "iopub.status.busy": "2025-01-14T01:55:32.858013Z",
     "iopub.status.idle": "2025-01-14T01:55:42.222800Z",
     "shell.execute_reply": "2025-01-14T01:55:42.222125Z",
     "shell.execute_reply.started": "2025-01-14T01:55:32.858200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"/kaggle/input/healthcare-chatbot/corpus_healthcare.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T01:55:42.224087Z",
     "iopub.status.busy": "2025-01-14T01:55:42.223937Z",
     "iopub.status.idle": "2025-01-14T01:55:47.220712Z",
     "shell.execute_reply": "2025-01-14T01:55:47.219846Z",
     "shell.execute_reply.started": "2025-01-14T01:55:42.224068Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-01ffa0a314a2>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số embeddings: 201278\n",
      "Embedding mẫu: tensor([ 2.4059e-02, -3.3965e-02,  2.7742e-02, -1.0075e-02,  4.8988e-02,\n",
      "        -1.9550e-02,  2.8276e-02,  6.6316e-03, -7.6046e-02, -3.5337e-02,\n",
      "        -2.5771e-02,  2.9420e-03, -3.3403e-02,  3.5558e-02,  2.1970e-02,\n",
      "         6.5235e-02,  3.8740e-02, -2.9432e-02,  1.5462e-02, -4.5378e-02,\n",
      "        -1.7632e-02, -4.9764e-02,  7.2799e-03, -1.3141e-02,  4.5410e-02,\n",
      "         6.4563e-03, -1.1320e-02, -6.8925e-02,  3.3045e-02,  3.9826e-02,\n",
      "        -4.1069e-02,  5.9556e-02, -4.6521e-02,  1.0189e-02,  3.5060e-02,\n",
      "        -9.0544e-04,  1.6474e-02,  5.6453e-02, -3.8884e-02,  1.8709e-02,\n",
      "         1.6357e-02, -4.0798e-04,  3.1813e-03,  5.2386e-03,  2.1488e-02,\n",
      "         2.4168e-02,  3.5207e-03,  6.2477e-02, -4.4913e-02,  6.8346e-03,\n",
      "         7.1183e-02, -4.1380e-02, -2.4523e-02, -1.8445e-02, -3.5180e-02,\n",
      "        -2.2221e-02,  4.8468e-02,  1.6551e-03,  3.7592e-02, -1.0580e-03,\n",
      "        -7.6505e-02,  2.0165e-02,  1.5811e-02, -1.8044e-02,  3.5696e-02,\n",
      "         2.5866e-02, -3.5296e-02, -6.0745e-02,  2.6852e-02,  2.2641e-02,\n",
      "        -6.9123e-03,  2.3974e-02, -2.8769e-02, -2.8555e-03, -1.2452e-02,\n",
      "         4.0721e-02,  1.6846e-02,  6.6493e-03, -5.7959e-02, -8.1682e-03,\n",
      "        -4.3219e-02, -1.4539e-02, -1.1943e-02,  7.1545e-02, -2.8127e-07,\n",
      "        -1.3317e-02, -1.5272e-02,  3.5358e-02, -1.1865e-02, -3.2631e-02,\n",
      "         6.4374e-03, -3.4729e-02,  2.3478e-02,  4.9086e-02, -1.3624e-02,\n",
      "        -5.1079e-02,  5.3677e-03, -3.5316e-02,  1.4896e-02,  3.7971e-02,\n",
      "         1.1257e-02,  2.3999e-04,  4.3988e-02, -7.9529e-02,  4.6767e-02,\n",
      "         4.5247e-03,  3.6682e-03, -5.6035e-04,  3.5647e-03,  4.4422e-02,\n",
      "        -1.8991e-02,  2.0673e-02,  1.1219e-02, -1.3208e-02,  3.9198e-02,\n",
      "         7.2597e-03,  7.4417e-02, -6.2402e-02,  8.1465e-03, -8.4038e-03,\n",
      "        -1.3039e-02,  5.7932e-03, -6.3941e-02,  4.5182e-03,  4.2233e-03,\n",
      "        -3.9931e-02, -2.8770e-02, -6.6802e-03,  3.2554e-02,  3.3766e-02,\n",
      "         7.8814e-02, -2.5786e-02, -1.4622e-02, -6.0537e-03, -4.6274e-02,\n",
      "        -1.3331e-02, -4.7901e-02,  4.1354e-02, -3.9761e-02,  1.7439e-02,\n",
      "        -3.4391e-03, -8.2077e-03, -4.4744e-02, -1.5636e-02, -6.9279e-03,\n",
      "        -9.0784e-03, -3.9576e-02,  1.7125e-02,  5.3428e-02,  2.4829e-02,\n",
      "        -1.4187e-03,  5.9862e-03,  2.5129e-02, -5.4371e-02,  2.0019e-02,\n",
      "        -1.1356e-02, -3.8008e-02, -2.0950e-02,  2.5377e-02,  3.1227e-02,\n",
      "        -3.3693e-02,  4.6230e-02, -1.8278e-02,  1.2947e-03,  2.4075e-02,\n",
      "        -5.7440e-02, -1.0504e-02,  1.5433e-02,  2.9203e-02,  8.8132e-03,\n",
      "         1.1481e-01,  2.3647e-02,  3.5831e-02,  2.9198e-03,  4.9956e-02,\n",
      "        -3.1491e-02, -2.3033e-02, -6.2112e-02, -5.3114e-03,  4.1124e-03,\n",
      "         1.7630e-02, -1.8766e-02,  5.5462e-02, -9.8888e-03, -2.2706e-02,\n",
      "         6.4787e-02, -4.4799e-04, -4.8775e-03, -2.1884e-02, -2.1811e-02,\n",
      "         7.4664e-02,  2.7595e-02,  3.3979e-02, -2.8670e-02, -1.3314e-02,\n",
      "        -7.8865e-03,  1.4405e-02, -3.6923e-02,  1.7461e-02, -3.3584e-02,\n",
      "        -1.0632e-02,  6.1364e-02, -3.4633e-02,  3.7900e-02,  3.9556e-02,\n",
      "        -1.0368e-03, -8.7173e-02, -2.9801e-02,  6.5475e-03, -5.4012e-03,\n",
      "        -6.6724e-03, -1.9535e-02, -2.0557e-02,  2.6616e-02, -3.2960e-02,\n",
      "        -9.8981e-03,  3.4997e-02, -2.4262e-02,  5.6989e-02, -2.7461e-03,\n",
      "         3.5114e-02, -1.2255e-02,  1.6489e-02, -1.7967e-02,  2.2610e-02,\n",
      "         1.4397e-02, -3.2192e-02,  5.7674e-02,  2.7409e-02, -2.1484e-02,\n",
      "        -1.8804e-02,  2.1353e-02,  2.4746e-02, -3.5801e-02, -2.5149e-02,\n",
      "        -2.7982e-02,  3.2851e-02, -4.1291e-02,  2.9179e-02, -4.3097e-02,\n",
      "        -1.5197e-02,  2.2526e-02, -2.2901e-03, -1.0235e-02,  3.9980e-02,\n",
      "        -3.5179e-03,  4.1038e-02,  5.7989e-02,  7.7357e-02,  9.1730e-02,\n",
      "        -3.9480e-02, -6.1419e-02,  5.6994e-02,  7.8949e-02,  6.9876e-04,\n",
      "        -2.4544e-02, -1.2270e-02,  5.9789e-03, -2.4678e-02, -1.3082e-02,\n",
      "        -6.4134e-02, -3.9895e-02, -1.1479e-02, -4.4548e-02, -3.8859e-02,\n",
      "        -3.4109e-02,  6.2473e-02,  1.9445e-02,  4.8033e-02,  3.8482e-02,\n",
      "        -6.0937e-02,  1.1985e-02,  5.5503e-02, -3.0002e-02, -2.6968e-02,\n",
      "         1.8439e-02, -2.3278e-02,  6.6806e-02,  5.8233e-02, -7.4002e-04,\n",
      "        -4.3635e-02, -4.5860e-02, -2.1453e-02, -3.5765e-02,  8.8075e-03,\n",
      "         5.3226e-02,  4.8517e-02,  5.8025e-04, -7.4628e-02,  2.6151e-03,\n",
      "        -1.1204e-02,  3.8504e-02, -5.2805e-03,  3.9137e-02, -2.5365e-02,\n",
      "         4.9498e-02,  3.1423e-02,  3.8458e-03, -2.1718e-02, -2.1361e-02,\n",
      "         2.5119e-03,  7.4608e-02,  4.4614e-03,  8.8275e-04,  5.0310e-02,\n",
      "        -3.7072e-02,  2.8007e-03, -2.5415e-02, -7.1036e-02, -2.2816e-02,\n",
      "        -4.7262e-02,  3.1421e-02, -1.3610e-02, -4.7913e-02,  6.3141e-02,\n",
      "         2.0874e-02, -1.0834e-01,  1.8909e-03,  6.1225e-02,  5.0491e-02,\n",
      "        -1.4733e-02,  3.1535e-02,  8.0464e-03, -1.8450e-02,  8.5646e-05,\n",
      "         2.6875e-02,  7.0965e-02, -1.7654e-02, -4.5255e-02, -1.2629e-03,\n",
      "         2.9562e-02, -2.0422e-02, -2.3187e-02,  3.0494e-02,  1.6927e-02,\n",
      "        -1.3241e-02,  1.2386e-02,  3.0686e-02, -3.6392e-02,  3.2671e-02,\n",
      "        -8.4191e-03, -3.8789e-02,  7.0490e-02,  3.3203e-02, -4.1450e-02,\n",
      "         3.5358e-02, -1.1785e-02, -5.3341e-04, -6.9502e-03,  1.7605e-02,\n",
      "        -3.0509e-02, -5.3329e-02, -2.4501e-02,  3.9574e-02,  5.1606e-03,\n",
      "         3.0889e-02, -1.3021e-02, -3.9268e-02, -2.9230e-02,  4.5270e-03,\n",
      "        -1.7042e-02,  4.2150e-02, -2.6333e-02, -2.4753e-02,  4.6327e-02,\n",
      "         5.6480e-03,  5.3624e-02, -9.6174e-03, -4.7516e-02, -7.9185e-04,\n",
      "        -1.7124e-02, -3.2295e-02,  2.8220e-02, -1.1602e-02, -2.7123e-02,\n",
      "         2.6491e-02,  1.3110e-02, -6.6848e-03,  2.0465e-02,  3.2286e-02,\n",
      "        -1.4458e-02,  4.1859e-03, -5.8181e-02,  7.2789e-03, -1.3365e-02,\n",
      "         4.9815e-02, -2.2747e-02,  9.4922e-03,  6.3908e-02, -6.2112e-03,\n",
      "         7.2490e-02,  8.6131e-02, -3.7743e-03, -3.7325e-02, -2.3757e-03,\n",
      "         2.6261e-02,  4.4865e-03, -1.7876e-02, -4.5882e-02,  3.9894e-02,\n",
      "        -6.6676e-02, -6.6491e-02,  7.8160e-03, -2.4329e-02,  6.9656e-02,\n",
      "         2.4666e-02, -2.6663e-02,  2.0374e-02, -3.9917e-02,  1.8769e-02,\n",
      "        -1.0194e-02, -4.3044e-02,  5.9719e-02, -5.2290e-02, -4.0543e-02,\n",
      "         1.9341e-02,  2.8554e-03,  6.5458e-03, -9.0315e-02,  1.8739e-02,\n",
      "         2.0726e-02,  2.4187e-02, -1.2880e-02, -4.0045e-03, -4.7575e-02,\n",
      "         3.9391e-03, -2.1876e-02,  2.1864e-02,  6.1211e-03, -4.2172e-02,\n",
      "         3.5360e-02, -7.3289e-02,  3.4566e-02, -4.2217e-04, -6.8396e-02,\n",
      "         2.0281e-02, -6.8580e-02,  2.0209e-02,  4.7860e-03, -4.9552e-02,\n",
      "         1.4285e-02,  5.8257e-02,  7.9863e-02, -3.7795e-02,  3.1750e-02,\n",
      "        -6.2171e-02, -2.1771e-02, -6.1555e-02,  3.3637e-02, -1.5762e-02,\n",
      "        -2.8643e-02, -1.0912e-02,  5.9314e-02, -4.1963e-03, -1.4664e-02,\n",
      "        -4.9062e-02,  2.1463e-02,  1.4402e-02, -2.2271e-02, -1.3468e-02,\n",
      "         6.4444e-02, -1.8013e-02, -3.4221e-02, -1.9271e-02, -4.0452e-02,\n",
      "        -2.4896e-02, -2.1598e-02,  1.0770e-02,  7.5467e-02, -2.2147e-02,\n",
      "        -2.8311e-02,  1.1068e-02, -6.6413e-03, -5.8525e-02, -1.3289e-02,\n",
      "        -2.3642e-03,  3.9665e-02, -4.0203e-02, -6.0659e-03, -3.2148e-02,\n",
      "        -5.5366e-02, -5.3701e-02,  2.0151e-02, -1.5394e-02, -1.5471e-03,\n",
      "        -1.9368e-03, -1.5635e-02,  3.4746e-02,  5.8698e-03, -2.3302e-02,\n",
      "        -7.1294e-03,  5.6211e-02, -1.1851e-02, -1.2827e-02, -2.4225e-03,\n",
      "        -3.8556e-03,  5.5684e-02, -2.3319e-02, -8.3555e-04,  9.4014e-02,\n",
      "         3.4620e-03, -3.5240e-02,  3.9648e-02, -4.3171e-04,  6.7995e-03,\n",
      "         2.9115e-02,  3.3952e-02,  6.7641e-02, -4.3100e-02, -3.4232e-02,\n",
      "        -1.2128e-02,  3.0190e-02,  1.0818e-02, -8.4519e-03, -3.4731e-03,\n",
      "         3.4418e-03, -2.4810e-02,  6.0652e-02, -2.0685e-02,  1.2689e-02,\n",
      "        -1.8979e-02, -6.2693e-02, -3.2045e-02,  2.2826e-02, -1.1369e-02,\n",
      "        -1.5014e-02, -6.3148e-02,  3.9678e-03, -5.3537e-02,  6.7086e-02,\n",
      "        -1.7183e-02, -5.9601e-03,  1.2418e-02, -4.4656e-02, -2.3720e-02,\n",
      "        -3.6326e-02,  5.4050e-02,  7.9749e-02,  6.9565e-03,  3.2636e-02,\n",
      "         5.2645e-02,  1.9164e-02,  6.4016e-02,  8.6404e-03,  1.5686e-02,\n",
      "        -1.4304e-02,  8.6608e-02, -2.0352e-02,  2.9414e-02,  1.1897e-02,\n",
      "         4.4854e-03,  6.5613e-03, -5.0672e-02, -4.8135e-02, -1.9980e-02,\n",
      "        -2.6516e-02, -2.2686e-02,  3.1132e-02, -2.2900e-02,  1.2423e-03,\n",
      "         4.0508e-02,  2.7795e-02,  4.8624e-02,  2.2573e-02,  1.6423e-02,\n",
      "        -6.4880e-03, -6.8463e-03,  6.6122e-02, -5.7403e-02, -6.6623e-03,\n",
      "         4.1057e-02,  3.0229e-02,  3.8762e-03,  4.7868e-02,  4.6292e-03,\n",
      "         4.4921e-03,  1.2285e-03,  3.1998e-02,  1.7849e-02, -1.7880e-02,\n",
      "         5.1928e-02, -3.6074e-02,  2.3998e-02,  1.5314e-02,  2.5100e-04,\n",
      "        -1.5793e-02, -2.5071e-02,  2.3710e-02,  2.3067e-02,  2.9669e-02,\n",
      "        -2.6013e-02,  4.2112e-02,  3.3522e-02, -6.8727e-02, -1.6226e-03,\n",
      "        -3.9230e-02, -1.4082e-02,  4.5484e-02,  2.9714e-02,  3.2423e-02,\n",
      "         4.5332e-02, -8.4504e-03, -1.2175e-02,  4.9675e-02,  4.9821e-03,\n",
      "         3.5361e-03,  5.0988e-02, -9.5496e-03, -1.0070e-02, -4.1873e-02,\n",
      "        -8.2437e-03, -2.1808e-02,  5.7837e-03,  1.2766e-02,  2.5432e-03,\n",
      "        -2.3144e-02, -5.0482e-02,  2.6386e-02,  9.4487e-03, -1.1225e-01,\n",
      "        -5.9313e-02,  1.9090e-02, -2.3671e-02, -4.2927e-02,  1.0432e-02,\n",
      "        -1.1602e-02,  1.9154e-02, -4.4334e-02,  5.8660e-03,  8.6105e-02,\n",
      "        -2.8478e-02,  5.6647e-02,  1.0667e-02, -3.7823e-02,  7.1633e-03,\n",
      "         6.2366e-02,  1.5526e-02, -1.5121e-02, -8.4968e-02,  2.1739e-02,\n",
      "        -7.2885e-03,  5.5380e-02, -8.2231e-03, -2.6244e-02, -2.7268e-02,\n",
      "        -6.1963e-02,  1.9665e-03,  2.2981e-02, -6.5161e-02, -3.5260e-02,\n",
      "        -1.2575e-02, -1.0736e-02, -9.5357e-03,  2.3233e-02, -1.7838e-02,\n",
      "         1.7580e-02,  1.4677e-03, -2.9921e-02, -1.3202e-02, -4.8280e-02,\n",
      "        -8.5397e-02,  1.9554e-02, -9.4044e-02,  2.2993e-02,  4.8353e-02,\n",
      "        -5.4981e-03, -1.7097e-02,  7.6700e-02, -9.0218e-02,  1.0128e-02,\n",
      "         9.5894e-03,  1.8218e-02, -5.5622e-03,  3.5029e-02, -3.1033e-02,\n",
      "        -7.0048e-03,  3.1154e-02, -2.0606e-02, -1.7473e-02,  2.0123e-02,\n",
      "        -4.0251e-02, -9.9812e-03,  8.0564e-03, -6.9934e-02, -1.3063e-02,\n",
      "         4.0817e-02,  2.6739e-02,  3.9085e-02, -2.2889e-02, -6.7201e-02,\n",
      "        -2.9152e-02,  8.0642e-02,  2.3707e-02, -4.6559e-02,  3.1527e-02,\n",
      "        -2.5573e-02,  6.6656e-02, -2.4770e-03, -7.1675e-04,  1.5986e-02,\n",
      "        -9.4783e-02, -6.9994e-03, -3.0101e-02,  3.7124e-03, -3.7860e-02,\n",
      "         1.4473e-02, -8.8885e-03,  4.3942e-03,  2.5849e-02, -1.0730e-02,\n",
      "         5.9455e-02, -5.2473e-03, -3.1628e-02, -1.3702e-02, -4.1185e-02,\n",
      "        -3.4216e-03,  3.3616e-02, -1.6137e-02,  1.1218e-01, -2.5451e-02,\n",
      "         5.9421e-03, -4.1967e-02, -1.1177e-02, -3.5199e-03,  2.8171e-02,\n",
      "        -7.9335e-03,  2.9104e-03, -3.4345e-03,  1.2468e-02, -6.8629e-02,\n",
      "         1.1623e-01,  6.5512e-04,  4.6722e-02, -3.8068e-02, -5.7454e-03,\n",
      "        -1.4838e-02, -2.8901e-03,  3.8278e-03,  6.2629e-04,  1.8935e-03,\n",
      "         3.4328e-02, -1.1855e-02, -8.3515e-03,  3.9418e-02, -5.1043e-03,\n",
      "        -2.4175e-02, -1.5353e-03, -1.6153e-02, -4.2538e-02,  1.6711e-02,\n",
      "         3.0812e-02, -1.9751e-02, -5.4960e-03, -1.9455e-02,  4.5958e-02,\n",
      "         7.4124e-02, -9.8125e-03, -2.7880e-02,  1.9816e-02,  1.1790e-02,\n",
      "        -4.1614e-02, -3.3883e-02, -1.0039e-02,  1.9087e-02,  2.8153e-02,\n",
      "         9.1467e-02, -2.4515e-02,  1.0974e-02], device='cuda:0')\n",
      "Kích thước của embedding: torch.Size([201278, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Đường dẫn đến tệp .pt\n",
    "file_path = \"/kaggle/input/embedding-doc/doc_embeddings_hl_ft.pt\"\n",
    "\n",
    "# Tải dữ liệu\n",
    "data = torch.load(file_path)\n",
    "\n",
    "# Kiểm tra và xử lý dữ liệu\n",
    "if isinstance(data, torch.Tensor):\n",
    "    # Nếu toàn bộ file là một tensor\n",
    "    embeddings = data\n",
    "elif isinstance(data, dict) and 'embeddings' in data:\n",
    "    # Nếu file là dictionary và chứa key 'embeddings'\n",
    "    embeddings = data['embeddings']\n",
    "else:\n",
    "    raise ValueError(\"File không chứa embeddings hoặc không đúng định dạng!\")\n",
    "\n",
    "# Hiển thị thông tin embeddings\n",
    "print(f\"Tổng số embeddings: {len(embeddings)}\")\n",
    "print(\"Embedding mẫu:\", embeddings[0])\n",
    "\n",
    "# Đảm bảo kiểu dữ liệu là tensor\n",
    "if isinstance(embeddings, torch.Tensor):\n",
    "    print(f\"Kích thước của embedding: {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T01:55:51.941306Z",
     "iopub.status.busy": "2025-01-14T01:55:51.941067Z",
     "iopub.status.idle": "2025-01-14T01:55:51.946556Z",
     "shell.execute_reply": "2025-01-14T01:55:51.945792Z",
     "shell.execute_reply.started": "2025-01-14T01:55:51.941279Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Replace newline characters with spaces\n",
    "    text = ' '.join(text.splitlines())\n",
    "    \n",
    "    # Replace escaped newline characters with periods\n",
    "    text = text.replace('\\\\n', '.')\n",
    "    \n",
    "    # Remove specific unwanted characters and patterns\n",
    "    text = text.replace('/', '')\n",
    "    text = text.replace(\"''\", '')\n",
    "    text = text.strip(\"[]\")\n",
    "    text = text.strip(\"'\")\n",
    "    text = text.strip('\"')\n",
    "    \n",
    "    # Remove numbered list indicators and lettered list indicators\n",
    "    text = re.sub(r'\\b\\d\\.\\s*', '', text)  # Removes patterns like '1. ', '2. ', etc.\n",
    "    text = re.sub(r'\\b[a-e]\\)\\s*', '', text)  # Removes patterns like 'a) ', 'b) ', etc.\n",
    "    \n",
    "    # Replace certain punctuation with periods\n",
    "    text = text.replace(';', '.')\n",
    "    text = text.replace(':', '.')\n",
    "    \n",
    "    # Remove hyphens\n",
    "    text = text.replace('-', '')\n",
    "    \n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Query đầu vào\n",
    "    query = \"Sốt xuất huyết ở trẻ em nguy hiểm thế nào khi không xử lý đúng cách?\"\n",
    "    \n",
    "    # Encode query bằng model\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # Tìm top-k văn bản liên quan nhất trong embeddings\n",
    "    top_k = 50\n",
    "    hits = util.semantic_search(query_embedding, embeddings, top_k=top_k)[0]\n",
    "    \n",
    "    # Hiển thị thông tin query\n",
    "    print(query)\n",
    "    print(\"\")\n",
    "    print(\"Context:\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # In top 10 kết quả dựa trên điểm bi-encoder (bi_score)\n",
    "    for hit in hits[:10]:\n",
    "        score = hit['score']\n",
    "        corpus_id = hit['corpus_id']\n",
    "        print(\"bi_score: \", score)\n",
    "        print(corpus.iloc[corpus_id]['context'])  # Truy cập cột context trong corpus\n",
    "        print(\" \")\n",
    "    \n",
    "    # Chuẩn bị input cho cross-encoder\n",
    "    # preprocess_text là hàm tiền xử lý nếu cần, nếu không cần thì giữ nguyên văn bản\n",
    "    cross_inp = [[query, corpus.iloc[hit['corpus_id']]['context']] for hit in hits]\n",
    "    \n",
    "    # Tính điểm cross-encoder\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "    \n",
    "    # Cập nhật điểm cross-score cho các hits\n",
    "    for idx in range(len(hits)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "    \n",
    "    # Sắp xếp hits theo cross-score (giảm dần)\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    \n",
    "    # In top 10 kết quả dựa trên điểm cross-encoder (cross_score)\n",
    "    for hit in hits[:10]:\n",
    "        score = hit['cross-score']\n",
    "        corpus_id = hit['corpus_id']\n",
    "        print(\"cross_score: \", score)\n",
    "        print(corpus.iloc[corpus_id]['context'])  # Truy cập cột context trong corpus\n",
    "        print(\" \")\n",
    "    \n",
    "    # Giải phóng bộ nhớ GPU (nếu cần)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T19:45:57.815452Z",
     "iopub.status.busy": "2025-01-13T19:45:57.815220Z",
     "iopub.status.idle": "2025-01-13T19:56:13.605365Z",
     "shell.execute_reply": "2025-01-13T19:56:13.604531Z",
     "shell.execute_reply.started": "2025-01-13T19:45:57.815429Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sentence_transformers import util\n",
    "\n",
    "def calculate_mrr(query_file, corpus_file, doc_embedding_file, model, cross_encoder, top_k=50):\n",
    "    \"\"\"\n",
    "    Tính Mean Reciprocal Rank (MRR) khi kết hợp cả bi-encoder và cross-encoder.\n",
    "\n",
    "    Parameters:\n",
    "    - query_file: Đường dẫn tới file chứa các truy vấn.\n",
    "    - corpus_file: Đường dẫn tới file chứa corpus.\n",
    "    - doc_embedding_file: Đường dẫn tới file chứa document embeddings.\n",
    "    - model: Bi-encoder model dùng để tính embeddings.\n",
    "    - cross_encoder: Cross-encoder model dùng để tính điểm cho các cặp (query, document).\n",
    "    - top_k: Số lượng văn bản được chọn từ bi-encoder để đưa vào cross-encoder.\n",
    "\n",
    "    Returns:\n",
    "    - mrr: Giá trị Mean Reciprocal Rank (float).\n",
    "    \"\"\"\n",
    "    reciprocal_ranks = []\n",
    "\n",
    "    # Load queries và corpus\n",
    "    query_df = pd.read_csv(query_file).head(100)  # Chỉ lấy 1000 query đầu tiên\n",
    "    corpus_df = pd.read_csv(corpus_file)\n",
    "\n",
    "    queries = query_df['query'].tolist()\n",
    "    corpus = corpus_df['context'].tolist()\n",
    "\n",
    "    # Load document embeddings\n",
    "    doc_embeddings = torch.load(doc_embedding_file)\n",
    "    doc_embeddings = doc_embeddings.cpu().numpy() if doc_embeddings.is_cuda else doc_embeddings.numpy()\n",
    "\n",
    "    for query_idx, query in enumerate(queries):\n",
    "        # Encode query bằng model\n",
    "        query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "        query_embedding = query_embedding.cpu().numpy() if query_embedding.is_cuda else query_embedding.numpy()\n",
    "\n",
    "        # Bi-encoder phase: Tính điểm similarity và lấy top-k văn bản liên quan nhất\n",
    "        scores = util.pytorch_cos_sim(query_embedding, doc_embeddings)\n",
    "        scores = scores.squeeze().numpy()\n",
    "        top_k_indices = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "        # Chuẩn bị input cho cross-encoder\n",
    "        cross_inp = [[query, corpus[idx]] for idx in top_k_indices]\n",
    "\n",
    "        # Cross-encoder phase: Tính điểm\n",
    "        cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "        # Sắp xếp lại theo điểm cross-score\n",
    "        ranked_indices = [top_k_indices[idx] for idx in np.argsort(cross_scores)[::-1]]\n",
    "\n",
    "        # Tìm vị trí của câu trả lời đúng\n",
    "        relevant_idx = query_idx  # Đồng bộ query và corpus\n",
    "        rank_position = next((idx + 1 for idx, doc_idx in enumerate(ranked_indices) if doc_idx == relevant_idx), None)\n",
    "\n",
    "        # Tính reciprocal rank\n",
    "        if rank_position is not None:\n",
    "            reciprocal_ranks.append(1 / rank_position)\n",
    "        else:\n",
    "            reciprocal_ranks.append(0)  # Nếu không tìm thấy câu trả lời đúng trong top-k\n",
    "\n",
    "    # Tính Mean Reciprocal Rank\n",
    "    mrr = np.mean(reciprocal_ranks)\n",
    "    return mrr\n",
    "\n",
    "# Đường dẫn tới file truy vấn, corpus và document embeddings\n",
    "query_file = \"/kaggle/input/testmrr/processed_queries.csv\"\n",
    "corpus_file = \"/kaggle/input/testmrr/corpus_with_index.csv\"\n",
    "doc_embedding_file = \"/kaggle/input/testmrr/doc_embeddings (1).pt\"\n",
    "\n",
    "# Giả sử model và cross_encoder đã được khởi tạo từ trước\n",
    "# model = ...  # Bi-encoder model\n",
    "# cross_encoder = ...  # Cross-encoder model\n",
    "\n",
    "# Kiểm tra khởi tạo model và cross_encoder (cần thay thế bằng mô hình thực tế)\n",
    "if 'model' not in locals() or 'cross_encoder' not in locals():\n",
    "    raise ValueError(\"Cần khởi tạo model và cross_encoder trước khi chạy chương trình.\")\n",
    "\n",
    "# Chạy hàm calculate_mrr\n",
    "mrr = calculate_mrr(query_file, corpus_file, doc_embedding_file, model, cross_encoder, top_k=50)\n",
    "print(f\"MRR: {mrr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T01:56:29.870588Z",
     "iopub.status.busy": "2025-01-14T01:56:29.870249Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from pyngrok import ngrok\n",
    "from flask_cors import CORS\n",
    "import requests  # Thư viện để gọi API HTTP\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# URL server ngrok thay vì sử dụng hàm `phogpt`\n",
    "NGROK_SERVER_URL =   # Thay bằng URL ngrok của bạn\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        # Lấy dữ liệu từ request.form\n",
    "        question = request.form.get('message')  # 'message' khớp với key trong JavaScript\n",
    "        if not question:\n",
    "            return jsonify({\"error\": \"Missing question\"}), 400\n",
    "\n",
    "        query_embedding = model.encode(question)\n",
    "        hits = util.semantic_search(query_embedding, embeddings, top_k=50)[0]\n",
    "        cross_inp = [[preprocess_text(question), preprocess_text(corpus.iloc[hit['corpus_id']]['context'])] for hit in hits]\n",
    "        cross_scores = cross_encoder.predict(cross_inp)\n",
    "        for idx in range(len(hits)):\n",
    "            hits[idx]['cross-score'] = cross_scores[idx]\n",
    "        hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "        best_hit = hits[0]\n",
    "        top_k_cross = corpus.iloc[best_hit['corpus_id']]['context']\n",
    "        print('cross-encoder')\n",
    "        \n",
    "        # Cross-encoder refinement\n",
    "        print(top_k_cross)\n",
    "        # Gọi tới server ngrok thay vì dùng phogpt\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{NGROK_SERVER_URL}/ask\",  # Đường dẫn tới endpoint xử lý trên server ngrok\n",
    "                json={\n",
    "                    \"question\": question,\n",
    "                    \"context\": top_k_cross  # Gửi thông tin cần thiết\n",
    "                }\n",
    "            )\n",
    "            response_data = response.json()\n",
    "\n",
    "            # Kiểm tra phản hồi từ server\n",
    "            if response.status_code != 200 or \"answer\" not in response_data:\n",
    "                return jsonify({\"answer\": \"Error processing the request to the external server.\"})\n",
    "            \n",
    "            answer = response_data[\"answer\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling external server: {str(e)}\")\n",
    "            return jsonify({\"answer\": \"Error contacting external server.\"})\n",
    "\n",
    "        if not answer:\n",
    "            return jsonify({\"answer\": \"Sorry, I couldn't generate an appropriate response.\"})\n",
    "        return jsonify({\"answer\": answer})\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log lỗi và trả về phản hồi JSON lỗi\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return jsonify({\"error\": f\"An error occurred: {str(e)}\"}), 500\n",
    "\n",
    "# Start the Flask server\n",
    "if __name__ == '__main__':\n",
    "    public_url = ngrok.connect(5000)  # Kết nối Flask server với ngrok\n",
    "    print(f\"Public URL: {public_url}\")\n",
    "    app.run(host='0.0.0.0', port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6420184,
     "sourceId": 10365783,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6423946,
     "sourceId": 10370958,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6425275,
     "sourceId": 10372744,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6425428,
     "sourceId": 10372960,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6470443,
     "sourceId": 10452671,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6471322,
     "sourceId": 10453928,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 208347,
     "modelInstanceId": 186229,
     "sourceId": 218375,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 209330,
     "modelInstanceId": 187251,
     "sourceId": 219562,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 215967,
     "modelInstanceId": 194051,
     "sourceId": 227609,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 215801,
     "modelInstanceId": 193885,
     "sourceId": 227611,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 216736,
     "modelInstanceId": 194835,
     "sourceId": 228485,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 216949,
     "modelInstanceId": 195053,
     "sourceId": 228738,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 217165,
     "modelInstanceId": 195272,
     "sourceId": 228994,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
